{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of News Articles Using NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"''\",\n",
       " \"'s\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '``',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '©',\n",
       " '—',\n",
       " '’',\n",
       " '“',\n",
       " '”'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a wordnet lemmatizer instance\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "#Getting all stopwords from corpus and including punctuations \n",
    "STOP_WORDS= set(stopwords.words(\"english\")+list(string.punctuation) + [\"’\",\"—\",'“','”','``',\"''\",'©',\"'s\" ])\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Reading prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>published</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.washingtonpost.com/news/worldviews...</td>\n",
       "      <td>Sun, 1 Jul 2018 10:03:49 GMT</td>\n",
       "      <td>Here’s what you need to know about Mexico’s pr...</td>\n",
       "      <td>\\n\\nMexican presidential candidate Andrés Manu...</td>\n",
       "      <td>Mexican presidential candidate Andrés Manuel L...</td>\n",
       "      <td>obrador, president, need, heres, know, trump, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.cnn.com/2018/07/01/asia/china-aust...</td>\n",
       "      <td>Sun, 1 Jul 2018 12:28:00 GMT</td>\n",
       "      <td>Thailand cave search: Divers close in on missi...</td>\n",
       "      <td>Chiang Rai (CNN) China and Australia have join...</td>\n",
       "      <td>Chiang Rai (CNN) China and Australia have join...</td>\n",
       "      <td>team, coach, close, rescue, missing, boys, cav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.yahoo.com/news/n-korea-aiming-hide...</td>\n",
       "      <td>Sun, 1 Jul 2018 10:15:23 GMT</td>\n",
       "      <td>N. Korea aiming to hide ongoing nuclear produc...</td>\n",
       "      <td>The assessment comes on the heels of a landmar...</td>\n",
       "      <td>Over the weekend NBC News first reported that ...</td>\n",
       "      <td>ongoing, weapons, n, nuclear, hide, washington...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.washingtonpost.com/news/worldviews...</td>\n",
       "      <td>Sat, 30 Jun 2018 22:41:15 GMT</td>\n",
       "      <td>Read U.S. ambassador to Estonia's resignation ...</td>\n",
       "      <td>\\n\\nJames D. Melville Jr. addresses dignitarie...</td>\n",
       "      <td>James D. Melville Jr. addresses dignitaries in...</td>\n",
       "      <td>president, resignation, melville, read, estoni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.yahoo.com/news/rebels-resume-peace...</td>\n",
       "      <td>Sun, 1 Jul 2018 11:35:00 GMT</td>\n",
       "      <td>Jordan seeks truce for southwest Syria after a...</td>\n",
       "      <td>Trucks loaded with humanitarian supplies to be...</td>\n",
       "      <td>Trucks loaded with humanitarian supplies to be...</td>\n",
       "      <td>seeks, rebel, towns, states, syria, army, unit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  \\\n",
       "0  https://www.washingtonpost.com/news/worldviews...   \n",
       "1  https://www.cnn.com/2018/07/01/asia/china-aust...   \n",
       "2  https://www.yahoo.com/news/n-korea-aiming-hide...   \n",
       "3  https://www.washingtonpost.com/news/worldviews...   \n",
       "4  https://www.yahoo.com/news/rebels-resume-peace...   \n",
       "\n",
       "                       published  \\\n",
       "0   Sun, 1 Jul 2018 10:03:49 GMT   \n",
       "1   Sun, 1 Jul 2018 12:28:00 GMT   \n",
       "2   Sun, 1 Jul 2018 10:15:23 GMT   \n",
       "3  Sat, 30 Jun 2018 22:41:15 GMT   \n",
       "4   Sun, 1 Jul 2018 11:35:00 GMT   \n",
       "\n",
       "                                               title  \\\n",
       "0  Here’s what you need to know about Mexico’s pr...   \n",
       "1  Thailand cave search: Divers close in on missi...   \n",
       "2  N. Korea aiming to hide ongoing nuclear produc...   \n",
       "3  Read U.S. ambassador to Estonia's resignation ...   \n",
       "4  Jordan seeks truce for southwest Syria after a...   \n",
       "\n",
       "                                                text  \\\n",
       "0  \\n\\nMexican presidential candidate Andrés Manu...   \n",
       "1  Chiang Rai (CNN) China and Australia have join...   \n",
       "2  The assessment comes on the heels of a landmar...   \n",
       "3  \\n\\nJames D. Melville Jr. addresses dignitarie...   \n",
       "4  Trucks loaded with humanitarian supplies to be...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Mexican presidential candidate Andrés Manuel L...   \n",
       "1  Chiang Rai (CNN) China and Australia have join...   \n",
       "2  Over the weekend NBC News first reported that ...   \n",
       "3  James D. Melville Jr. addresses dignitaries in...   \n",
       "4  Trucks loaded with humanitarian supplies to be...   \n",
       "\n",
       "                                            keywords  sentiment  \n",
       "0  obrador, president, need, heres, know, trump, ...          0  \n",
       "1  team, coach, close, rescue, missing, boys, cav...          1  \n",
       "2  ongoing, weapons, n, nuclear, hide, washington...          0  \n",
       "3  president, resignation, melville, read, estoni...          0  \n",
       "4  seeks, rebel, towns, states, syria, army, unit...          0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading dataset with sentiment classification and \n",
    "master=pd.read_csv(\"../Datasets/master_sentiment.csv\")\n",
    "# Dropping 17 and 36 as they have NaN articles\n",
    "master_rem_na=master.drop([17,36])\n",
    "master_rem_na.reset_index(inplace=True, drop=True)\n",
    "master_rem_na.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is to convert from wordet POS_Tag to WordNetLemmatizer format\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    else:\n",
    "        return wn.NOUN\n",
    "\n",
    "# This function counts the articles with the same title and removes the repeated articles.   \n",
    "def remove_repeat_articles(articles): \n",
    "    count_and_index=dict()\n",
    "    for index, row in articles.iterrows():\n",
    "        try:\n",
    "            count_and_index[ row[\"title\"] ][\"count\"]+=1\n",
    "            count_and_index[ row[\"title\"] ][\"indices\"].append(index)\n",
    "        except:\n",
    "            count_and_index[ row[\"title\"] ]=dict()\n",
    "            count_and_index[ row[\"title\"] ][\"count\"]=1\n",
    "            count_and_index[ row[\"title\"] ][\"indices\"]=[index]\n",
    "    droplist=list()\n",
    "    for key, element in count_and_index.items():\n",
    "        if element[\"count\"]>1:\n",
    "            droplist.extend( element[\"indices\"][1:] )\n",
    "  \n",
    "    master_clean = articles.drop(master.index[droplist])\n",
    "    master_clean.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return master_clean\n",
    "  \n",
    "\n",
    "master_remove_repeat = remove_repeat_articles(master_rem_na)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preliminary Processing (Tokenization and Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function tokenizes title and article into words and removes stop words and punctutations\n",
    "def tokenization_remove_stopwords(text):\n",
    "    words_tokenized= word_tokenize(text)\n",
    "    words_sans_stopwords = [w.lower() for w in words_tokenized if w not in STOP_WORDS]\n",
    "    return words_sans_stopwords\n",
    "#Finding pos_tag for each word and lemmatization using pos_tag as an attribute\n",
    "def pos_lemmatization(text):\n",
    "    text_pos=pos_tag(text)\n",
    "    text_wn_pos=[(x,get_wordnet_pos(y)) for x,y  in text_pos]\n",
    "    text_lemmatized = [ lemmatizer.lemmatize(x,pos=y) for x,y in text_wn_pos]\n",
    "    return text_lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, we get the processed text and title. We then club them together in a tuple using the zip function.\n",
    "\n",
    "# First we tokenize and remove stopwords and punctuations\n",
    "text_tokenized =master_remove_repeat[\"text\"].apply(tokenization_remove_stopwords) \n",
    "title_tokenized =  master_remove_repeat[\"title\"].apply(tokenization_remove_stopwords) \n",
    "labels=master_remove_repeat[\"sentiment\"]\n",
    "# Then we lemmatize the processed sentences\n",
    "title_lemmatized = [pos_lemmatization(title) for title in title_tokenized]\n",
    "text_lemmatized = [pos_lemmatization(article) for article in text_tokenized]\n",
    "\n",
    "# We zip the title and text into tuples\n",
    "title_text_labels_lemmatized=list( zip(title_lemmatized, text_lemmatized, labels) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function that outputs a list of all words in the dataset\n",
    "def all_words(lemmatized_dataset):\n",
    "    return [word for text in lemmatized_dataset for word in text]\n",
    "\n",
    "\n",
    "text_lemmatized_all = all_words(text_lemmatized)\n",
    "title_lemmatized_all= all_words(title_lemmatized)\n",
    "text_lemmatized_all_freq = nltk.FreqDist(text_lemmatized_all)\n",
    "word_features= [x for x,y in text_lemmatized_all_freq.most_common(3000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that finds feature values in a document\n",
    "def find_features(document):\n",
    "    words= set(document)\n",
    "    features={}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating featureset using article words and labels \n",
    "# and classifying whether article contains the word in word_features (True or False)\n",
    "featuresets=[(find_features(text), label) for (title, text, label) in title_text_labels_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shuffling featuresets\n",
    "np.random.seed(111)\n",
    "np.random.shuffle(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preparing traing\n",
    "training_set=featuresets[:1200]\n",
    "testing_set=featuresets[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Algo accuracy percent: 93.75\n"
     ]
    }
   ],
   "source": [
    "classifier= nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier,training_set) ) *100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               agreement = True                0 : 1      =    168.4 : 1.0\n",
      "                      eu = True                0 : 1      =    116.6 : 1.0\n",
      "                 spiegel = True                0 : 1      =     80.3 : 1.0\n",
      "               tolerance = True                0 : 1      =     75.1 : 1.0\n",
      "                    u.s. = True                0 : 1      =     74.2 : 1.0\n",
      "                  import = True                0 : 1      =     70.0 : 1.0\n",
      "                  merkel = True                0 : 1      =     70.0 : 1.0\n",
      "                  senate = True                0 : 1      =     64.8 : 1.0\n",
      "                  export = True                0 : 1      =     64.8 : 1.0\n",
      "              right-wing = True                0 : 1      =     59.6 : 1.0\n",
      "                     der = True                0 : 1      =     59.6 : 1.0\n",
      "             policewoman = True                0 : 1      =     54.4 : 1.0\n",
      "               afp/getty = True                0 : 1      =     54.4 : 1.0\n",
      "               manhattan = True                0 : 1      =     54.4 : 1.0\n",
      "              chancellor = True                0 : 1      =     52.2 : 1.0\n",
      "          administration = True                0 : 1      =     51.6 : 1.0\n",
      "                democrat = True                0 : 1      =     51.3 : 1.0\n",
      "                  detain = True                0 : 1      =     49.2 : 1.0\n",
      "                brussels = True                0 : 1      =     49.2 : 1.0\n",
      "               detention = True                0 : 1      =     49.2 : 1.0\n",
      "                 obrador = True                0 : 1      =     49.2 : 1.0\n",
      "                  balkan = True                0 : 1      =     49.2 : 1.0\n",
      "                   rival = True                0 : 1      =     48.2 : 1.0\n",
      "            conservative = True                0 : 1      =     45.5 : 1.0\n",
      "              obligation = True                0 : 1      =     45.1 : 1.0\n",
      "                   putin = True                0 : 1      =     45.1 : 1.0\n",
      "                  repeal = True                0 : 1      =     45.1 : 1.0\n",
      "             negotiation = True                0 : 1      =     45.1 : 1.0\n",
      "            increasingly = True                0 : 1      =     45.1 : 1.0\n",
      "                 volcano = True                0 : 1      =     44.1 : 1.0\n",
      "                 erdogan = True                0 : 1      =     44.1 : 1.0\n",
      "                  awaits = True                0 : 1      =     44.1 : 1.0\n",
      "                    oust = True                0 : 1      =     44.1 : 1.0\n",
      "                currency = True                0 : 1      =     44.1 : 1.0\n",
      "              separatist = True                0 : 1      =     44.1 : 1.0\n",
      "                  cartel = True                0 : 1      =     44.1 : 1.0\n",
      "                 theresa = True                0 : 1      =     44.1 : 1.0\n",
      "              democratic = True                0 : 1      =     43.3 : 1.0\n",
      "               coalition = True                0 : 1      =     43.3 : 1.0\n",
      "                 dispute = True                0 : 1      =     42.0 : 1.0\n",
      "                  speaks = True                0 : 1      =     42.0 : 1.0\n",
      "                 digital = True                1 : 0      =     40.4 : 1.0\n",
      "                   trump = True                0 : 1      =     39.5 : 1.0\n",
      "        israeli-occupied = True                0 : 1      =     38.9 : 1.0\n",
      "               jerusalem = True                0 : 1      =     38.9 : 1.0\n",
      "                davidson = True                0 : 1      =     38.9 : 1.0\n",
      "                  tayyip = True                0 : 1      =     38.9 : 1.0\n",
      "                     eid = True                0 : 1      =     38.9 : 1.0\n",
      "              rebel-held = True                0 : 1      =     38.9 : 1.0\n",
      "                sentence = True                0 : 1      =     38.9 : 1.0\n",
      "                   golan = True                0 : 1      =     38.9 : 1.0\n",
      "                firework = True                0 : 1      =     38.9 : 1.0\n",
      "                    lava = True                0 : 1      =     38.9 : 1.0\n",
      "                    jong = True                0 : 1      =     38.9 : 1.0\n",
      "              venezuelan = True                0 : 1      =     38.9 : 1.0\n",
      "                   recep = True                0 : 1      =     38.9 : 1.0\n",
      "                   medic = True                0 : 1      =     38.9 : 1.0\n",
      "             resignation = True                0 : 1      =     38.9 : 1.0\n",
      "              protestors = True                0 : 1      =     38.9 : 1.0\n",
      "            displacement = True                0 : 1      =     38.9 : 1.0\n",
      "                   belch = True                0 : 1      =     38.9 : 1.0\n",
      "                republic = True                0 : 1      =     38.9 : 1.0\n",
      "                  brexit = True                0 : 1      =     38.9 : 1.0\n",
      "             palestinian = True                0 : 1      =     38.9 : 1.0\n",
      "                  impose = True                0 : 1      =     36.6 : 1.0\n",
      "                  arabia = True                0 : 1      =     36.6 : 1.0\n",
      "                   saudi = True                0 : 1      =     36.6 : 1.0\n",
      "              corruption = True                0 : 1      =     36.6 : 1.0\n",
      "             immigration = True                0 : 1      =     36.0 : 1.0\n",
      "                  summit = True                0 : 1      =     36.0 : 1.0\n",
      "               reporting = True                0 : 1      =     35.8 : 1.0\n",
      "                  macron = True                0 : 1      =     35.4 : 1.0\n",
      "                  mexico = True                0 : 1      =     34.6 : 1.0\n",
      "                    iran = True                0 : 1      =     34.4 : 1.0\n",
      "                 consent = True                0 : 1      =     34.4 : 1.0\n",
      "                zimbabwe = True                0 : 1      =     33.7 : 1.0\n",
      "                     yas = True                0 : 1      =     33.7 : 1.0\n",
      "                   liege = True                0 : 1      =     33.7 : 1.0\n",
      "                  afghan = True                0 : 1      =     33.7 : 1.0\n",
      "           no-confidence = True                0 : 1      =     33.7 : 1.0\n",
      "                 shrader = True                0 : 1      =     33.7 : 1.0\n",
      "                 call-in = True                0 : 1      =     33.7 : 1.0\n",
      "                  maduro = True                0 : 1      =     33.7 : 1.0\n",
      "                 vatican = True                0 : 1      =     33.7 : 1.0\n",
      "                 catalan = True                0 : 1      =     33.7 : 1.0\n",
      "                 gassama = True                0 : 1      =     33.7 : 1.0\n",
      "                   agung = True                0 : 1      =     33.7 : 1.0\n",
      "                 al-fitr = True                0 : 1      =     33.7 : 1.0\n",
      "                      fe = True                0 : 1      =     33.7 : 1.0\n",
      "                 caracas = True                0 : 1      =     33.7 : 1.0\n",
      "                ministry = True                0 : 1      =     33.2 : 1.0\n",
      "                  ashraf = True                0 : 1      =     32.6 : 1.0\n",
      "                publicly = True                0 : 1      =     32.6 : 1.0\n",
      "                  bolton = True                0 : 1      =     32.6 : 1.0\n",
      "              republican = True                0 : 1      =     32.2 : 1.0\n",
      "                 senator = True                0 : 1      =     32.2 : 1.0\n",
      "                   favor = True                0 : 1      =     32.2 : 1.0\n",
      "                 migrant = True                0 : 1      =     31.7 : 1.0\n",
      "                    ally = True                0 : 1      =     30.5 : 1.0\n",
      "                 defense = True                0 : 1      =     30.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11670313639679067"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "160/1371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
